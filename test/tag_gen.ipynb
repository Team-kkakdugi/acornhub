{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2256d596",
   "metadata": {},
   "source": [
    "## 카드 생성 시 테그 생성 알고리즘\n",
    "\n",
    "NER or 토픽 모델링 사용 예정!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a4194d",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873e1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyBhLmkl8pch1-aUmi3VsCsDRDYYjCgI2Dk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9b3cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔비디아(NVIDIA)가 AI 시대에 강세를 보이는 이유는 단순히 뛰어난 하드웨어를 만들어서만이 아니라, 여러 복합적이고 전략적인 요인들 때문입니다. 핵심적인 이유들은 다음과 같습니다.\n",
      "\n",
      "1.  **압도적인 GPU(그래픽 처리 장치) 성능과 병렬 처리 능력:**\n",
      "    *   **AI의 본질과 GPU의 적합성:** AI, 특히 딥러닝 모델은 수많은 계산(행렬 연산 등)을 동시에 병렬적으로 처리해야 합니다. CPU(중앙 처리 장치)는 순차적 처리에 능하지만, GPU는 수천 개의 작은 코어를 가지고 있어 이러한 병렬 연산에 최적화되어 있습니다.\n",
      "    *   **엔비디아의 기술력:** 엔비디아는 수십 년간 게이밍 그래픽 시장을 통해 GPU 기술을 발전시켜 왔으며, AI 시대에 필요한 대규모 병렬 컴퓨팅에 특화된 GPU 아키텍처(예: Ampere, Hopper)와 Tensor Cores 같은 전용 AI 가속 기능을 개발하여 성능을 극대화했습니다. 현재 AI 칩 시장에서 엔비디아의 A100, H100 같은 GPU는 사실상의 표준으로 자리 잡고 있습니다.\n",
      "\n",
      "2.  **독보적인 소프트웨어 생태계 'CUDA':**\n",
      "    *   **진정한 경쟁 우위:** 엔비디아의 가장 강력한 무기 중 하나는 하드웨어가 아닌 소프트웨어, 바로 'CUDA(Compute Unified Device Architecture)'입니다. CUDA는 엔비디아 GPU를 사용하여 병렬 컴퓨팅 애플리케이션을 개발할 수 있는 플랫폼이자 프로그래밍 모델입니다.\n",
      "    *   **개발자 락인(Lock-in) 효과:** 수십 년간 축적된 CUDA 생태계는 AI 연구자와 개발자들에게 익숙하고 강력한 도구, 라이브러리(cuDNN, cuBLAS 등), 프레임워크(TensorFlow, PyTorch 등)와의 높은 호환성을 제공합니다. 대다수의 AI 모델과 알고리즘이 CUDA를 기반으로 개발되고 최적화되어 있습니다. 경쟁사들이 엔비디아와 유사한 성능의 하드웨어를 만든다고 해도, 이 방대한 소프트웨어 생태계를 단기간에 따라잡기 매우 어렵습니다. 이는 엔비디아의 독점적인 지위를 공고히 하는 핵심 요인입니다.\n",
      "\n",
      "3.  **선견지명과 전략적 투자:**\n",
      "    *   **미래 예측:** 엔비디아는 일찍이 GPU가 그래픽 처리뿐만 아니라 일반적인 병렬 컴퓨팅, 특히 AI에 엄청난 잠재력을 가지고 있음을 간파했습니다. 2000년대 중반부터 GPU 컴퓨팅을 학계와 연구 커뮤니티에 적극적으로 홍보하고 지원하며 AI 기술 발전에 기여했습니다.\n",
      "    *   **지속적인 R&D:** AI 연구와 개발에 막대한 투자를 통해 최신 AI 알고리즘과 기술에 최적화된 하드웨어와 소프트웨어를 지속적으로 선보이고 있습니다.\n",
      "\n",
      "4.  **포괄적인 솔루션 제공:**\n",
      "    *   **하드웨어 + 소프트웨어 + 네트워크:** 엔비디아는 단순히 GPU 칩만 파는 것이 아닙니다. GPU가 탑재된 서버, 데이터센터용 시스템, 고속 네트워킹(InfiniBand 기술을 보유한 Mellanox 인수), AI 개발 및 배포를 위한 포괄적인 소프트웨어 스택(SDKs, 라이브러리, MLOps 툴 등) 등 AI 구현에 필요한 모든 솔루션을 제공합니다.\n",
      "    *   **엔드 투 엔드(End-to-End) 지원:** 이는 고객들이 엔비디아 제품을 통해 '엔드 투 엔드' AI 솔루션을 손쉽게 구축하고 운영할 수 있도록 돕습니다.\n",
      "\n",
      "5.  **AI 모델 '학습(Training)' 시장의 독점적 지위:**\n",
      "    *   현재 거대 AI 모델(LLM, 이미지 생성 모델 등)을 학습시키는 데는 엔비디아의 GPU가 필수적이며, 거의 독점적인 지위를 가지고 있습니다. AI 기술이 발전하고 모델의 규모가 커질수록 엔비디아 GPU의 수요는 폭발적으로 증가하고 있습니다.\n",
      "\n",
      "결론적으로, 엔비디아는 강력한 병렬 처리 하드웨어(GPU), 독보적인 소프트웨어 생태계(CUDA), 그리고 미래를 내다본 전략적 투자와 포괄적인 솔루션 제공을 통해 AI 시대의 핵심 동력으로 확고히 자리매김했습니다. AI 기술의 발전과 적용이 가속화될수록 엔비디아의 중요성은 더욱 커질 것으로 예상됩니다.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents = \"엔비디아가 AI 시대에 강세를 보이는 이유에 대해 설명해주세요.\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a8dab",
   "metadata": {},
   "source": [
    "## 카드에서 테그 추출 알고리즘\n",
    "\n",
    "NER + 토픽 모델링 테스트 해볼거에용~\n",
    "자료 원문 텍스트만 받아서, 테그 3개~5개 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71cfe871",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from konlpy) (2.3.4)\n",
      "Requirement already satisfied: packaging in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4298681e",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from keybert) (2.3.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from keybert) (14.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from keybert) (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from keybert) (5.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn>=0.22.2->keybert) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn>=0.22.2->keybert) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (2.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sentence-transformers>=0.3.8->keybert) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a22ae7f",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e1ad0c",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c191d9d6",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a04fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ff0343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# --- 모델 초기화 ---\n",
    "# Okt(Open Korean Text) 형태소 분석기 초기화\n",
    "try:\n",
    "    okt = Okt()\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Okt: {e}\")\n",
    "    print(\"Please ensure KoNLPy and an appropriate Java Development Kit (JDK) are installed.\")\n",
    "    okt = None\n",
    "\n",
    "# KeyBERT 모델 초기화 (다국어 지원 모델 사용)\n",
    "try:\n",
    "    # 'distiluse-base-multilingual-cased'는 다양한 언어에서 준수한 성능을 보이는 경량화된 모델입니다.\n",
    "    kw_model = KeyBERT('distiluse-base-multilingual-cased')\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing KeyBERT: {e}\")\n",
    "    print(\"Please ensure 'keybert', 'torch', and 'transformers' are installed.\")\n",
    "    kw_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = \"\"\"\n",
    "인공지능(AI) 기술이 빠르게 발전하면서, 자연어 처리(NLP) 분야도 큰 변화를 맞이하고 있습니다.\n",
    "최근 구글, 오픈AI 등 빅테크 기업들은 초거대 AI 모델을 연이어 발표했습니다.\n",
    "이러한 모델들은 작문, 번역, 코딩 등 다양한 영역에서 인간 수준의 능력을 보여주며\n",
    "산업 전반에 걸쳐 혁신을 주도하고 있습니다. 하지만 모델의 편향성 문제와\n",
    "환경 비용에 대한 우려도 함께 제기되고 있어, 책임감 있는 AI 개발이 중요한 화두로 떠올랐습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd89ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    간단한 텍스트 전처리 함수 (특수문자, 불필요한 공백 제거)\n",
    "    \"\"\"\n",
    "    # 한글, 영어, 숫자, 공백을 제외한 모든 문자 제거\n",
    "    text = re.sub(r'[^가-힣A-Za-z0-9\\s]', '', text)\n",
    "    # 연속된 공백을 하나의 공백으로 변경\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def get_tags_by_frequency(text, n_tags=5):\n",
    "    \"\"\"\n",
    "    방법 1: 명사 추출 및 빈도수 기반 태그 생성 (NER과 유사한 접근)\n",
    "    \"\"\"\n",
    "    if not okt:\n",
    "        print(\"Okt not initialized. Skipping frequency-based tagging.\")\n",
    "        return []\n",
    "        \n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # 텍스트에서 명사만 추출\n",
    "    nouns = okt.nouns(cleaned_text)\n",
    "    \n",
    "    # 한 글자 명사 제외\n",
    "    filtered_nouns = [n for n in nouns if len(n) > 1]\n",
    "    \n",
    "    # 빈도수 계산\n",
    "    count = Counter(filtered_nouns)\n",
    "    \n",
    "    # 가장 빈번하게 등장한 명사 n_tags개 반환\n",
    "    common_tags = count.most_common(n_tags)\n",
    "    \n",
    "    return [tag for tag, freq in common_tags]\n",
    "\n",
    "def get_tags_by_keybert_with_nouns(text, n_tags=5):\n",
    "    \"\"\"\n",
    "    방법 2: KeyBERT와 명사 후보군을 이용한 문맥 기반 태그 생성 (토픽 모델링과 유사한 접근)\n",
    "    \"\"\"\n",
    "    if not okt or not kw_model:\n",
    "        print(\"Okt or KeyBERT not initialized. Skipping KeyBERT-based tagging.\")\n",
    "        return []\n",
    "\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # 1. 명사 추출 (키워드 후보군으로 사용)\n",
    "    nouns = okt.nouns(cleaned_text)\n",
    "    candidates = list(set([n for n in nouns if len(n) > 1])) # 중복 제거\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"No valid noun candidates found for KeyBERT.\")\n",
    "        return []\n",
    "        \n",
    "    # 2. KeyBERT를 사용하여 원본 텍스트와 가장 관련도 높은 후보(명사) 추출\n",
    "    # keyphrase_ngram_range=(1, 1) : 1개 단어로 이루어진 키워드만 추출\n",
    "    try:\n",
    "        keywords = kw_model.extract_keywords(cleaned_text,\n",
    "                                           candidates=candidates,\n",
    "                                           keyphrase_ngram_range=(1, 1),\n",
    "                                           stop_words=None,\n",
    "                                           top_n=n_tags)\n",
    "        \n",
    "        return [tag for tag, score in keywords]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during KeyBERT extraction: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d82620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 태그 생성 스크립트 ---\n",
      "입력 텍스트:\n",
      "\n",
      "인공지능(AI) 기술이 빠르게 발전하면서, 자연어 처리(NLP) 분야도 큰 변화를 맞이하고 있습니다.\n",
      "최근 구글, 오픈AI 등 빅테크 기업들은 초거대 AI 모델을 연이어 발표했습니다.\n",
      "이러한 모델들은 작문, 번역, 코딩 등 다양한 영역에서 인간 수준의 능력을 보여주며\n",
      "산업 전반에 걸쳐 혁신을 주도하고 있습니다. 하지만 모델의 편향성 문제와\n",
      "환경 비용에 대한 우려도 함께 제기되고 있어, 책임감 있는 AI 개발이 중요한 화두로 떠올랐습니다.\n",
      "\n",
      "\n",
      "[방법 1: 빈도수 기반 태그 (상위 10개)]\n",
      "['모델', '인공', '지능', '기술', '발전', '자연어', '처리', '분야', '변화', '맞이']\n",
      "--------------------\n",
      "[방법 2: KeyBERT 문맥 기반 태그 (상위 10개)]\n",
      "['코딩', '번역', '인간', '구글', '최근', '자연어', '산업', '환경', '대한', '작문']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- 태그 생성 스크립트 ---\")\n",
    "    print(f\"입력 텍스트:\\n{SAMPLE_TEXT}\\n\")\n",
    "    \n",
    "    # 방법 1: 빈도수 기반 태그\n",
    "    print(f\"[방법 1: 빈도수 기반 태그 (상위 10개)]\")\n",
    "    tags_freq = get_tags_by_frequency(SAMPLE_TEXT, n_tags=10)\n",
    "    print(tags_freq)\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # 방법 2: KeyBERT (문맥) 기반 태그\n",
    "    print(f\"[방법 2: KeyBERT 문맥 기반 태그 (상위 10개)]\")\n",
    "    tags_bert = get_tags_by_keybert_with_nouns(SAMPLE_TEXT, n_tags=10)\n",
    "    print(tags_bert)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d88ae766",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b0ab862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/oli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/oli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/oli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/oli/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/oli/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/oli/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk colab 환경에서 실행시 필요한 코드입니다. \n",
    "# 토큰화, 품사 태깅, 개체명 인식에 필요한 리소스 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3beddf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk \n",
    "sentence = 'James is working at Disney in London' \n",
    "tokenized_sentence = pos_tag(word_tokenize(sentence)) \n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0ca134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON James/NNP)\n",
      "  is/VBZ\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Disney/NNP)\n",
      "  in/IN\n",
      "  (GPE London/NNP))\n"
     ]
    }
   ],
   "source": [
    "ner_sentence = ne_chunk(tokenized_sentence) \n",
    "print(ner_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8007e138",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (3.8.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (8.3.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.3.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/oli/olis_space/backend/.venv/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f638497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b5c7d9",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:1034: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5a7569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buyin at U.K startup for $1 billion.\n",
      "[Apple, is, looking, at, buyin, at, U.K, startup, for, $, 1, billion, .]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # 원하는 언어의 모델을 가져옴 \n",
    "doc = nlp('Apple is looking at buyin at U.K startup for $1 billion.') # 문장을 nlp 에 넘겨주면 됨 \n",
    "print(doc) # 원래 문장이 출력됨 \n",
    "print(list(doc)) #리스트로 변형하면 토큰화한 결과가 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d989cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion') \n",
    "\n",
    "for ent in doc.ents : \n",
    "  print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2150e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Apple’s Siri ORG\n",
      "iPhones ORG\n",
      "Amazon ORG\n",
      "Alexa ORG\n",
      "Echo LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\".replace(\"\\n\", \" \").strip())\n",
    "\n",
    "## 아래처럼 무엇이 organization이고, 무엇이 product인지, 꽤 잘 구별해주지만, \n",
    "## echo, dot 등에 대해서는 정확하지 못하다. \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "kobert-ner-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Ko-NER model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "\n",
      "Input text: 철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\n",
      "\n",
      "NER Results:\n"
     ]
    }
   ],
   "source": [
    "# KoBERT-NER 예제\n",
    "# 필요한 라이브러리 설치 (이미 설치되어 있다면 빠르게 넘어갑니다)\n",
    "!pip install -q torch transformers seqeval\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# \"ner\" 파이프라인을 사용하여 미리 학습된 모델 로드\n",
    "# 모델: monologg/kocharelectra-base-modu-ner-all\n",
    "print(\"Loading Ko-NER model...\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"monologg/kocharelectra-base-modu-ner-all\", aggregation_strategy=\"simple\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 테스트할 한국어 문장\n",
    "text = \"철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\"\n",
    "\n",
    "# NER 실행\n",
    "print(f\"\\nInput text: {text}\")\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nNER Results:\")\n",
    "for entity in ner_results:\n",
    "    print(f\"  - Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "kobert-ner-debug",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Tokenizer & Model ---\n",
      "Tokenizer & Model Loaded.\n",
      "-----------------------------------\n",
      "--- 2. Tokenization Details ---\n",
      "Input Text: 철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\n",
      "Tokens: ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '.', '[SEP]']\n",
      "-----------------------------------\n",
      "--- 3. Raw Model Predictions ---\n",
      "Shape of predictions: torch.Size([1, 10])\n",
      "Prediction IDs: [0 0 0 0 0 0 0 0 0 0]\n",
      "-----------------------------------\n",
      "--- 4. Per-Token Label Results ---\n",
      "[CLS]           -> O\n",
      "[UNK]           -> O\n",
      "[UNK]           -> O\n",
      "[UNK]           -> O\n",
      "[UNK]           -> O\n",
      ",               -> O\n",
      "[UNK]           -> O\n",
      "[UNK]           -> O\n",
      ".               -> O\n",
      "[SEP]           -> O\n"
     ]
    }
   ],
   "source": [
    "# KoBERT-NER 수동 디버깅\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# 1. 토크나이저와 모델 각각 로드\n",
    "print(\"--- 1. Loading Tokenizer & Model ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kocharelectra-base-modu-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"monologg/kocharelectra-base-modu-ner-all\")\n",
    "print(\"Tokenizer & Model Loaded.\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "text = \"철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\"\n",
    "\n",
    "# 2. 문장을 토큰으로 나누고 ID로 변환\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "print(\"--- 2. Tokenization Details ---\")\n",
    "print(f\"Input Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "\n",
    "# 3. 모델 추론 실행\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "print(\"--- 3. Raw Model Predictions ---\")\n",
    "print(\"Shape of predictions:\", predictions.shape)\n",
    "print(\"Prediction IDs:\", predictions[0].numpy())\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"--- 4. Per-Token Label Results ---\")\n",
    "# 4. 각 토큰별 예측 결과 확인\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    label = model.config.id2label[prediction]\n",
    "    print(f\"{token:<15} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aef533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading ElectraTokenizer & Model ---\n",
      "...Done\n",
      "\n",
      "--- Running NER for: '철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.' ---\n",
      "Tokenization Check: ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '.', '[SEP]']\n",
      "\n",
      "--- Final Results (Text 1) ---\n",
      "\n",
      "--- Running NER for: '이재용 삼성전자 회장이 다음 주 미국으로 출장을 떠난다.' ---\n",
      "Tokenization Check: ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.', '[SEP]']\n",
      "\n",
      "--- Final Results (Text 2) ---\n"
     ]
    }
   ],
   "source": [
    "# KoBERT-NER 최종 해결책 v2: ElectraTokenizer 직접 사용\n",
    "from transformers import ElectraTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# 1. ElectraTokenizer를 명시적으로 사용하여 토크나이저 로드\n",
    "print(\"--- Loading ElectraTokenizer & Model ---\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/kocharelectra-base-modu-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"monologg/kocharelectra-base-modu-ner-all\")\n",
    "print(\"...Done\")\n",
    "\n",
    "def kobert_ner_final_solution(text):\n",
    "    \"\"\"\n",
    "    KoBERT-NER 모델을 ElectraTokenizer로 실행하고, 결과를 단어 단위로 묶어주는 함수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running NER for: '{text}' ---\")\n",
    "    \n",
    "    # 2. 토큰화 및 추론\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    print(f\"Tokenization Check: {tokens}\") # 토큰화 결과 확인\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "    # 3. 결과 후처리 및 단어 단위로 묶기\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, token_prediction in enumerate(predictions[0]):\n",
    "        label = model.config.id2label[token_prediction.item()]\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"word\": token.replace(\"##\", \"\"), \"label\": label[2:]}\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            if current_entity[\"label\"] == label[2:]:\n",
    "                current_entity[\"word\"] += token.replace(\"##\", \"\")\n",
    "            else:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = {\"word\": token.replace(\"##\", \"\"), \"label\": label[2:]}\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "                \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "        \n",
    "    return entities\n",
    "\n",
    "# --- 실행 ---\n",
    "text1 = \"철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\"\n",
    "text2 = \"이재용 삼성전자 회장이 다음 주 미국으로 출장을 떠난다.\"\n",
    "\n",
    "results1 = kobert_ner_final_solution(text1)\n",
    "print(\"\\n--- Final Results (Text 1) ---\")\n",
    "for entity in results1:\n",
    "    print(f\"  - Entity: {entity['word']}, Label: {entity['label']}\")\n",
    "\n",
    "results2 = kobert_ner_final_solution(text2)\n",
    "print(\"\\n--- Final Results (Text 2) ---\")\n",
    "for entity in results2:\n",
    "    print(f\"  - Entity: {entity['word']}, Label: {entity['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "978a57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "415e5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.cache/huggingface/hub/models--monologg--kocharelectra-base-modu-ner-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0279c042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading KLUE NER Model (soddokayo/klue-roberta-large-klue-ner) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Model Loaded Successfully!\n",
      "\n",
      "Input: 철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\n",
      "\n",
      "--- NER Results ---\n",
      "  - Entity: 철수, Label: PS, Score: 0.9903\n",
      "  - Entity: 서울역, Label: LC, Score: 0.7596\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers \"tokenizers>=0.13.3\" \"sentencepiece>=0.1.91\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 2. KLUE NER 파이프라인 로드 (정확한 모델 이름 사용)\n",
    "try:\n",
    "    print(\"--- Loading KLUE NER Model (soddokayo/klue-roberta-large-klue-ner) ---\")\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"soddokayo/klue-roberta-large-klue-ner\", aggregation_strategy=\"simple\")\n",
    "    print(\"...Model Loaded Successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model loading: {e}\")\n",
    "    ner_pipeline = None\n",
    "\n",
    "# 3. 실행 및 결과 확인\n",
    "if ner_pipeline:\n",
    "    text = \"철수는 서울역에서 비빔밥을 먹고, 삼성전자에 출근했다.\"\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    \n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    print(\"\\n--- NER Results ---\")\n",
    "    if ner_results:\n",
    "        for entity in ner_results:\n",
    "            print(f\"  - Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc06cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_contents = \"Node.js는 JavaScript 코드를 브라우저 밖에서 실행할 수 있게 해주는 런타임 환경이다.  빈번한 I/O 처리에 있어서의 우수한 성능[2], 서버 확장의 용이성, 무엇보다도 JavaScript라는 프론트엔드 필수 언어로 백엔드까지 작성할 수 있다는 엄청난 장점 때문에 출시 이후로 빠르게 점유율을 높여가고 있다.  2009년 5월 27일 처음 출시되었으며, 오픈 소스 JavaScript 엔진인 크롬 V8에 비동기 이벤트 처리 라이브러리인 libuv를 결합하여 구현되었다. Ryan Dahl이 처음 개발했으며,[3] 처음엔 리눅스와 macOS만 지원되었으나 2011년 7월에 Windows 버전도 발표되었다.  2014년 12월 한때 Node.js의 포크인 io.js가 나타나면서 Node.js 0.12 버전, io.js 3.3 버전까지 서로 분열된 모습으로 이어지는 듯했지만, 2015년 9월에 Node.js 4.0 버전으로 병합되어 현재에 이르렀다.  최신 버전은 기능이 불안정하거나 일부 모듈[4](패키지)이 작동하지 않을 수 있으므로 안정성을 원한다면 LTS 버전을 사용하는 게 좋다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7f815e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 라이브러리 임포트 완료 ---\n",
      "Gemini LLM 클라이언트 초기화 완료.\n",
      "Okt 형태소 분석기 초기화 완료.\n",
      "KeyBERT 모델 초기화 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1369: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLUE NER 모델 초기화 완료.\n",
      "--- 모든 모델 초기화 완료 ---\n",
      "\n",
      "--- 태그 후보 (총 43개) ---\n",
      "['js', '5월', '런타임', '성능', '빈번', '2009년5월27일', '2011년', '2011년7월', 'RyanDahl', '장점', '2', '3.3', '서버', '어서', 'js의', '최신', '처음', '이르렀다', '언어', '용이', '0.12', '브라우저', '실행', '2009년', '오픈', '2014년12월', '27일', '필수', '백엔드', '환경', '4', '버전', '2015년9월', '처리', '출시', '프론트엔드', '4.0', '무엇', '코드', '3', '확장', '크롬', '12월']\n",
      "\n",
      "--- Gemini-2.5-flash에게 최적 태그 선택/정리 요청 (5개) ---\n",
      "\n",
      "====================\n",
      " 최종 생성 태그\n",
      "====================\n",
      "['Node.js', 'JavaScript', '버전', '성능', '서버']\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai konlpy keybert \"transformers>=4.31.0\" \"tokenizers>=0.13.3\" \"sentencepiece>=0.1.91\"\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import google.genai as genai\n",
    "from konlpy.tag import Okt\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"--- 라이브러리 임포트 완료 ---\")\n",
    "\n",
    "# --- 2. 모델 초기화 ---\n",
    "\n",
    "# Gemini LLM 클라이언트\n",
    "try:\n",
    "    client = genai.Client(api_key=\"AIzaSyBhLmkl8pch1-aUmi3VsCsDRDYYjCgI2Dk\")\n",
    "    print(\"Gemini LLM 클라이언트 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gemini LLM 클라이언트 초기화 실패: {e}\")\n",
    "    client = None\n",
    "\n",
    "# 명사 빈도수 분석 및 구문 분석용 Okt\n",
    "try:\n",
    "    okt = Okt()\n",
    "    print(\"Okt 형태소 분석기 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"Okt 초기화 실패: {e}\")\n",
    "    okt = None\n",
    "\n",
    "# KeyBERT 모델\n",
    "try:\n",
    "    kw_model = KeyBERT('distiluse-base-multilingual-cased')\n",
    "    print(\"KeyBERT 모델 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"KeyBERT 초기화 실패: {e}\")\n",
    "    kw_model = None\n",
    "\n",
    "# KLUE NER 모델\n",
    "try:\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"soddokayo/klue-roberta-large-klue-ner\", aggregation_strategy=\"simple\")\n",
    "    print(\"KLUE NER 모델 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"KLUE NER 초기화 실패: {e}\")\n",
    "    ner_pipeline = None\n",
    "    \n",
    "print(\"--- 모든 모델 초기화 완료 ---\")\n",
    "\n",
    "\n",
    "# --- 3. 태그 후보 추출 함수들 (품질 개선) ---\n",
    "\n",
    "def get_tags_by_frequency(text, n_tags=20): # 후보 수 증가\n",
    "    if not okt: return []\n",
    "    cleaned_text = re.sub(r'[^가-힣A-Za-z0-9\\s]', '', text)\n",
    "    nouns = okt.nouns(cleaned_text)\n",
    "    filtered_nouns = [n for n in nouns if len(n) > 1]\n",
    "    count = Counter(filtered_nouns)\n",
    "    return [tag for tag, freq in count.most_common(n_tags)]\n",
    "\n",
    "def get_tags_by_keybert_and_phrases(text, n_tags=20): # 후보 수 증가 및 로직 개선\n",
    "    if not kw_model or not okt: return []\n",
    "    \n",
    "    # 1. Okt를 사용해 의미있는 명사구(연속된 명사)와 단일 명사 모두 추출\n",
    "    phrases = okt.phrases(text)\n",
    "    nouns = [n for n in okt.nouns(text) if len(n) > 1]\n",
    "    \n",
    "    # 2. 후보군 통합 (중복 제거)\n",
    "    candidates = list(set(phrases + nouns))\n",
    "    \n",
    "    if not candidates:\n",
    "        return []\n",
    "        \n",
    "    # 3. KeyBERT로 후보군 중에서 본문과 가장 관련도 높은 키워드 랭킹 선정\n",
    "    keywords = kw_model.extract_keywords(text, candidates=candidates, top_n=n_tags)\n",
    "    return [tag for tag, score in keywords]\n",
    "\n",
    "def get_tags_by_ner(text):\n",
    "    if not ner_pipeline: return []\n",
    "    ner_results = ner_pipeline(text)\n",
    "    # NER 결과에서 'word'만 추출\n",
    "    return [entity['word'].replace(\" \", \"\") for entity in ner_results]\n",
    "\n",
    "\n",
    "# --- 4. LLM을 이용한 최종 태그 선택 함수 (gemini-pro 사용) ---\n",
    "\n",
    "def generate_final_tags_with_llm(card_content, max_tags=5):\n",
    "    if not client:\n",
    "        print(\"LLM 클라이언트가 초기화되지 않아 태그 생성을 건너뜁니다.\")\n",
    "        return []\n",
    "\n",
    "    # 1단계: 개선된 방법론으로 태그 후보 추출\n",
    "    freq_tags = get_tags_by_frequency(card_content)\n",
    "    keyphrase_tags = get_tags_by_keybert_and_phrases(card_content)\n",
    "    ner_tags = get_tags_by_ner(card_content)\n",
    "    \n",
    "    # 2단계: 모든 후보를 합치고 중복 제거\n",
    "    candidate_tags = list(set(freq_tags + keyphrase_tags + ner_tags))\n",
    "    \n",
    "    print(f\"\\n--- 태그 후보 (총 {len(candidate_tags)}개) ---\")\n",
    "    print(candidate_tags)\n",
    "    \n",
    "    if not candidate_tags:\n",
    "        print(\"추출된 태그 후보가 없습니다.\")\n",
    "        return []\n",
    "        \n",
    "    # 3단계: LLM에게 보낼 프롬프트\n",
    "    prompt = f\"\"\"\n",
    "    다음은 특정 문서에서 추출한 태그 후보 목록입니다. 이 목록은 문서의 핵심 내용을 담고 있습니다.\n",
    "\n",
    "    --- 태그 후보 목록 ---\n",
    "    {', '.join(candidate_tags)}\n",
    "    --------------------\n",
    "    \n",
    "    이 목록을 보고, 문서의 핵심 주제를 가장 잘 나타내는 최종 태그를 {max_tags}개만 골라 다듬어주세요.\n",
    "    예를 들어, '인공지능'과 'AI'가 둘 다 있다면 'AI'로 합치거나, 너무 광범위한 단어는 제거할 수 있습니다.\n",
    "    결과는 반드시쉼표(,)로만 구분된 리스트 형태로 답해주세요. (예: 태그1,태그2,태그3)\n",
    "    만약 적절한 태그가 없다면 \"없음\"이라고 답해주세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 4단계: LLM 호출 (gemini-pro 사용)\n",
    "    print(f\"\\n--- Gemini-2.5-flash에게 최적 태그 선택/정리 요청 ({max_tags}개) ---\")\n",
    "    try:\n",
    "        response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)\n",
    "        \n",
    "        if \"없음\" in response.text:\n",
    "            return []\n",
    "            \n",
    "        final_tags = [tag.strip() for tag in response.text.split(',')]\n",
    "        return final_tags\n",
    "    except Exception as e:\n",
    "        print(f\"LLM 호출 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 5. 실행 ---\n",
    "card_text = \"\"\"\n",
    "Node.js는 JavaScript 코드를 브라우저 밖에서 실행할 수 있게 해주는 런타임 환경이다.  빈번한 I/O 처리에 있어서의 우수한 성능[2], 서버 확장의 용이성, 무엇보다도 JavaScript라는 프론트엔드 필수 언어로 백엔드까지 작성할 수 있다는 엄청난 장점 때문에 출시 이후로 빠르게 점유율을 높여가고 있다.  2009년 5월 27일 처음 출시되었으며, 오픈 소스 JavaScript 엔진인 크롬 V8에 비동기 이벤트 처리 라이브러리인 libuv를 결합하여 구현되었다. Ryan Dahl이 처음 개발했으며,[3] 처음엔 리눅스와 macOS만 지원되었으나 2011년 7월에 Windows 버전도 발표되었다.  2014년 12월 한때 Node.js의 포크인 io.js가 나타나면서 Node.js 0.12 버전, io.js 3.3 버전까지 서로 분열된 모습으로 이어지는 듯했지만, 2015년 9월에 Node.js 4.0 버전으로 병합되어 현재에 이르렀다.  최신 버전은 기능이 불안정하거나 일부 모듈[4](패키지)이 작동하지 않을 수 있으므로 안정성을 원한다면 LTS 버전을 사용하는 게 좋다.\n",
    "\"\"\"\n",
    "\n",
    "final_tags = generate_final_tags_with_llm(card_text, max_tags=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*20)\n",
    "print(\" 최종 생성 태그\")\n",
    "print(\"=\"*20)\n",
    "print(final_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e81b3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 라이브러리 임포트 완료 ---\n",
      "Gemini LLM 클라이언트 초기화 완료.\n",
      "Okt 형태소 분석기 초기화 완료.\n",
      "KeyBERT 모델 초기화 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/oli/olis_space/backend/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1369: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLUE NER 모델 초기화 완료.\n",
      "--- 모든 모델 초기화 완료 ---\n",
      "\n",
      "--- 1. 태그 후보 추출 중... ---\n",
      "\n",
      "--- 2. 태그 후보 종합 (총 51개) ---\n",
      "['모델들은', '편향성', '하지만 모델의', '기술이 빠르게', '이재용', '번역', '기술', '작문', '모델을 연이어', '인공지능', '구글', '변화', '맞이', '인공', '기업', '변화를 맞이하고', '오픈AI', '발전', '삼성전자', '코딩 다양한', '인재', '구글 오픈ai', '산업', '발표', '빠르게 발전하면서', '대한', '모델', '오픈', '연이어', '최근', '초거대', '혁신을 주도하고', '환경', '테크', '코딩', '거대', '처리', '분야', '모델들은 작문', '번역 코딩', '이러한 모델들은', '책임감', '걸쳐 혁신을', '최근 구글', '지능', '분야도 변화를', '영역에서 인간', '자연어', '발전하면서 자연어', '인간', '모델의 편향성']\n",
      "\n",
      "--- 3. Gemini-Pro에게 최적 태그 선택/정리 요청 (5개) ---\n",
      "\n",
      "====================\n",
      " 최종 생성 태그\n",
      "====================\n",
      "['초거대 AI 모델', '기술 혁신', '산업 변화', '빅테크 기업', 'AI 윤리']\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai konlpy keybert \"transformers>=4.31.0\" \"tokenizers>=0.13.3\" \"sentencepiece>=0.1.91\"\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import google.genai as genai\n",
    "from konlpy.tag import Okt\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"--- 라이브러리 임포트 완료 ---\")\n",
    "\n",
    "# --- 2. 모델 초기화 ---\n",
    "\n",
    "# Gemini LLM 클라이언트\n",
    "try:\n",
    "    client = genai.Client(api_key=\"AIzaSyBhLmkl8pch1-aUmi3VsCsDRDYYjCgI2Dk\")\n",
    "    print(\"Gemini LLM 클라이언트 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gemini LLM 클라이언트 초기화 실패: {e}\")\n",
    "    client = None\n",
    "\n",
    "# Okt\n",
    "try:\n",
    "    okt = Okt()\n",
    "    print(\"Okt 형태소 분석기 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"Okt 초기화 실패: {e}\")\n",
    "    okt = None\n",
    "\n",
    "# KeyBERT\n",
    "try:\n",
    "    kw_model = KeyBERT('distiluse-base-multilingual-cased')\n",
    "    print(\"KeyBERT 모델 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"KeyBERT 초기화 실패: {e}\")\n",
    "    kw_model = None\n",
    "\n",
    "# KLUE NER\n",
    "try:\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"soddokayo/klue-roberta-large-klue-ner\", aggregation_strategy=\"simple\")\n",
    "    print(\"KLUE NER 모델 초기화 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"KLUE NER 초기화 실패: {e}\")\n",
    "    ner_pipeline = None\n",
    "    \n",
    "print(\"--- 모든 모델 초기화 완료 ---\")\n",
    "\n",
    "\n",
    "# --- 3. 태그 후보 추출 함수들 (모든 방법 총동원) ---\n",
    "\n",
    "def get_tags_by_frequency(text, n_tags=20):\n",
    "    if not okt: return []\n",
    "    nouns = okt.nouns(re.sub(r'[^가-힣A-Za-z0-9\\s]', '', text))\n",
    "    return [n for n, c in Counter(n for n in nouns if len(n) > 1).most_common(n_tags)]\n",
    "\n",
    "def get_tags_by_keybert_ngrams(text, n_tags=20):\n",
    "    if not kw_model: return []\n",
    "    return [tag for tag, score in kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=n_tags)]\n",
    "\n",
    "def get_tags_by_okt_phrases(text, n_tags=20):\n",
    "    if not kw_model or not okt: return []\n",
    "    phrases = okt.phrases(text)\n",
    "    if not phrases: return []\n",
    "    return [tag for tag, score in kw_model.extract_keywords(text, candidates=phrases, top_n=n_tags)]\n",
    "\n",
    "def get_tags_by_ner(text):\n",
    "    if not ner_pipeline: return []\n",
    "    return [entity['word'].replace(\" \", \"\") for entity in ner_pipeline(text)]\n",
    "\n",
    "\n",
    "# --- 4. LLM을 이용한 최종 태그 선택 함수 ---\n",
    "\n",
    "def generate_final_tags_with_llm(card_content, max_tags=5):\n",
    "    if not client:\n",
    "        print(\"LLM 클라이언트가 초기화되지 않아 태그 생성을 건너뜁니다.\")\n",
    "        return []\n",
    "\n",
    "    # 1단계: 모든 방법론으로 태그 후보 추출\n",
    "    print(\"\\n--- 1. 태그 후보 추출 중... ---\")\n",
    "    freq_tags = get_tags_by_frequency(card_content)\n",
    "    ngram_tags = get_tags_by_keybert_ngrams(card_content)\n",
    "    phrase_tags = get_tags_by_okt_phrases(card_content)\n",
    "    ner_tags = get_tags_by_ner(card_content)\n",
    "    \n",
    "    # 2단계: 모든 후보를 합치고 중복 제거\n",
    "    candidate_tags = list(set(freq_tags + ngram_tags + phrase_tags + ner_tags))\n",
    "    \n",
    "    print(f\"\\n--- 2. 태그 후보 종합 (총 {len(candidate_tags)}개) ---\")\n",
    "    print(candidate_tags)\n",
    "    \n",
    "    if not candidate_tags:\n",
    "        print(\"추출된 태그 후보가 없습니다.\")\n",
    "        return []\n",
    "        \n",
    "    # 3단계: LLM에게 보낼 프롬프트\n",
    "    prompt = f\"\"\"\n",
    "    다음은 특정 문서에서 다양한 알고리즘으로 추출한 태그 후보 목록입니다.\n",
    "\n",
    "    --- 태그 후보 목록 ---\n",
    "    {', '.join(candidate_tags)}\n",
    "    --------------------\n",
    "    \n",
    "    이 목록을 보고, 문서의 핵심 주제를 가장 잘 나타내는 최종 태그를 {max_tags}개만 골라 다듬어주세요.\n",
    "    예를 들어, '인공지능'과 'AI'가 둘 다 있다면 'AI'로 합치고, 너무 광범위하거나 중요하지 않은 단어는 제거해주세요.\n",
    "    결과는 반드시쉼표(,)로만 구분된 리스트 형태로 답해주세요. (예: 태그1,태그2,태그3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 4단계: LLM 호출\n",
    "    print(f\"\\n--- 3. Gemini-Pro에게 최적 태그 선택/정리 요청 ({max_tags}개) ---\")\n",
    "    try:\n",
    "        response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)\n",
    "        final_tags = [tag.strip() for tag in response.text.split(',')]\n",
    "        return final_tags\n",
    "    except Exception as e:\n",
    "        print(f\"LLM 호출 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- 5. 실행 ---\n",
    "card_text = \"\"\"\n",
    "인공지능(AI) 기술이 빠르게 발전하면서, 자연어 처리(NLP) 분야도 큰 변화를 맞이하고 있습니다.\n",
    "최근 구글, 오픈AI 등 빅테크 기업들은 초거대 AI 모델을 연이어 발표했습니다.\n",
    "이러한 모델들은 작문, 번역, 코딩 등 다양한 영역에서 인간 수준의 능력을 보여주며\n",
    "산업 전반에 걸쳐 혁신을 주도하고 있습니다. 하지만 모델의 편향성 문제와\n",
    "환경 비용에 대한 우려도 함께 제기되고 있어, 책임감 있는 AI 개발이 중요한 화두로 떠올랐습니다.\n",
    "이재용 삼성전자 회장은 AI 인재 확보의 중요성을 강조했습니다.\n",
    "\"\"\"\n",
    "\n",
    "final_tags = generate_final_tags_with_llm(card_text, max_tags=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*20)\n",
    "print(\" 최종 생성 태그\")\n",
    "print(\"=\"*20)\n",
    "print(final_tags)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
